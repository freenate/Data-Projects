---
title: "R Notebook"
output:
   rmarkdown::github_document
   
---

First thing to do is load the data into R, then combine the tables but save the training index to avoid having to possibly transform two tables instead of just one.  Finally let's look at the combined table.

```{r}
library(glmnet)
library(glmnetUtils)
library(knitr)
library(MASS)
library(ggplot2)
library(car)
library(caret)


train <- read.csv("train.csv")
test <- read.csv("test.csv")
train.Id <- train$Id
test.Id <- test$Id
train$Id <- NULL
test$Id <- NULL
test$SalePrice <- NA
Houses <- rbind(train,test)
summary(Houses)
str(Houses)
```
Remove features that have very low variance or a large amount of NAs and that also look not very useful.
```{r}
Houses$Street <- NULL
Houses$Alley <- NULL
Houses$Utilities <- NULL
Houses$Condition2 <- NULL
Houses$RoofMatl <- NULL
Houses$BsmtCond <- NULL
Houses$BsmtFinSF2 <- NULL
Houses$Heating <- NULL
Houses$CentralAir <- NULL
Houses$LowQualFinSF <- NULL
Houses$BsmtHalfBath <- NULL
Houses$Functional <- NULL
Houses$FireplaceQu <- NULL
Houses$GarageQual <- NULL
Houses$GarageCond <- NULL
Houses$PoolQC <- NULL
Houses$Fence <- NULL



```

Lot Frontage varies considerably by neighborhood, so imputing the missing data based on the mean lot frontage per neighborhood seems reasonable.
```{r}

ggplot(Houses, aes(x=Neighborhood, y=LotFrontage)) + stat_summary(fun.y="mean", geom="bar") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

```{r}
for(i in 1:nrow(Houses)){
  if(is.na(Houses$LotFrontage[i])){
    Houses$LotFrontage[i] <- mean(Houses$LotFrontage[Houses$Neighborhood==Houses$Neighborhood[i]], na.rm=T)
  }}

```
We see that for the 23 rows with NAs in MasVnrArea, each of them also had an NA entry for MasVnrType.  This leaves us with only one NA MasVnrType entry where the input for MasMasVnrArea is not an NA.  We will impute the mode, which is "None" and 0 for MasVnrType and MasVnrArea respectively for the 23 entires with NAs in both slots and the mode for all values that are _not_ "None" for the one other NA, which is "BrkFace".  
```{r}
Houses[is.na(Houses$MasVnrType),]
```
```{r}
Houses$MasVnrType[is.na(Houses$MasVnrType)&!is.na(Houses$MasVnrArea)] <- "BrkFace"
Houses$MasVnrType[is.na(Houses$MasVnrType)] <- "None"
Houses$MasVnrArea[is.na(Houses$MasVnrArea)] <- 0
```
For houses without a basement, we impute the missing values for BsmtExposure, BsmtExposure, BsmtFinType1, BsmtFinType2 with "No Bsmt"
```{r}
Houses[Houses$TotalBsmtSF==0,]
```
```{r}
levels(Houses$BsmtQual) <- c(levels(Houses$BsmtQual),"No Bsmt")
levels(Houses$BsmtExposure) <- c(levels(Houses$BsmtExposure),"No Bsmt")
levels(Houses$BsmtFinType1) <- c(levels(Houses$BsmtFinType1),"No Bsmt")
levels(Houses$BsmtFinType2) <- c(levels(Houses$BsmtFinType2),"No Bsmt")

Houses$BsmtQual[Houses$TotalBsmtSF==0] <- "No Bsmt"
Houses$BsmtExposure[Houses$TotalBsmtSF==0] <- "No Bsmt"
Houses$BsmtFinType1[Houses$TotalBsmtSF==0] <- "No Bsmt"
Houses$BsmtFinType2[Houses$TotalBsmtSF==0] <- "No Bsmt"
```
For all the other NA entries for these four predictors, we will impute the mode.
```{r}
Houses[is.na(Houses$BsmtQual)|is.na(Houses$BsmtExposure)|is.na(Houses$BsmtFinType1)|is.na(Houses$BsmtFinType2),]
```
```{r}
Houses$BsmtQual[is.na(Houses$BsmtQual)] <- "TA"
Houses$BsmtExposure[is.na(Houses$BsmtExposure)] <- "No"
Houses$BsmtFinType1[is.na(Houses$BsmtFinType1)] <- "Unf"
Houses$BsmtFinType2[is.na(Houses$BsmtFinType2)] <- "Unf"
```
First let's impute the obvious values for NAs in GarageType and GarageFinish.
```{r}
levels(Houses$GarageType) <- c(levels(Houses$GarageType), "No Garage")
levels(Houses$GarageFinish) <- c(levels(Houses$GarageFinish), "No Garage")

Houses$GarageType[Houses$GarageArea==0] <- "No Garage"
Houses$GarageFinish[Houses$GarageArea==0] <- "No Garage"
```
For the other two NAs in GarageFinish where the house actually has a garage, we see that they both have the value "Detchd" for GarageType. Upon using a table, we see that by far the most common value for this type of garage is "Unf".
```{r}
Houses[is.na(Houses$GarageFinish),]
```
```{r}
table(Houses$GarageType, Houses$GarageFinish)
```
```{r}
Houses$GarageFinish[is.na(Houses$GarageFinish)] <- "Unf"
```
For houses with no garage, we impute 0 for GarageYrblt and insert a dummy variable for whether the house has a garage or not. Also for houses with a Garage but that have an NA value for GarageYrBlt, we can simply impute the year the house was built.
```{r}
Houses$GarageYrBlt[is.na(Houses$GarageYrBlt) & Houses$GarageType != "No Garage"] <- Houses$YearBuilt[is.na(Houses$GarageYrBlt) & Houses$GarageType != "No Garage"]
```
```{r}
Houses$GarageYrBlt[is.na(Houses$GarageYrBlt)] <- 0
```
```{r}
Houses$HasGarage <- factor(ifelse(Houses$GarageType != "No Garage", "yes", "no"), levels = c("yes", "no"))
```
Looking at the summary, there is an obvious mistake in a GarageYrBlt entry. Since it seems like the intention was to input 2007 and that was also when the house was last remodeled, it makes sense for us to replace the error with 2007.
```{r}
Houses[Houses$GarageYrBlt == 2207,]
```
```{r}
Houses$GarageYrBlt[Houses$GarageYrBlt == 2207] <- 2007
```
For the MiscFeature variable, simply replace the NAs with "None".
```{r}
levels(Houses$MiscFeature) <- c(levels(Houses$MiscFeature), "None")
Houses$MiscFeature[is.na(Houses$MiscFeature)] <- "None"
```
Finally, the following variables each have only a handful of NAs at most and such we can simply impute them with the mean if numeric or mode if categorical.
```{r}
Houses$MSZoning[is.na(Houses$MSZoning)] <- "RL"
Houses$Exterior1st[is.na(Houses$Exterior1st)] <- "VinylSd"
Houses$Exterior2nd[is.na(Houses$Exterior2nd)] <- "VinylSd"
Houses$BsmtFinSF1[is.na(Houses$BsmtFinSF1)] <- 441.4
Houses$BsmtUnfSF[is.na(Houses$BsmtUnfSF)] <- 560.8
Houses$TotalBsmtSF[is.na(Houses$TotalBsmtSF)] <- 1051.8
Houses$Electrical[is.na(Houses$Electrical)] <- "SBrkr"
Houses$BsmtFullBath[is.na(Houses$BsmtFullBath)] <- 0
Houses$KitchenQual[is.na(Houses$KitchenQual)] <- "TA"
Houses$GarageCars[is.na(Houses$GarageCars)] <- 2
Houses$GarageArea[is.na(Houses$GarageArea)] <- 472.9
Houses$SaleType[is.na(Houses$SaleType)] <- "WD"

```
from the [pdf](http://jse.amstat.org/v19n3/decock.pdf) associated with this dataset, we are informed that there are five outliers that have GrLiveArea > 4000. We can remove the four that are in the training set, but not the one in the test set.
```{r}
plot(Houses$GrLivArea, Houses$SalePrice)
Houses[Houses$GrLivArea > 4000,]
```
```{r}
Houses <- Houses[-c(524, 692, 1183, 1299),]
train.Id <- which(!is.na(Houses$SalePrice))
```

A histogram shows that the response is negatively skewed, so a transformation would be necessary to make it more like a normal distribution. Log(natural logarithm) should be sufficient. *It is very important to undo the transformation at the end!*.
```{r}
ggplot(data=Houses[train.Id,], aes(x=Houses$SalePrice[train.Id])) + geom_histogram(col="red", fill = "green")
```
```{r}
ggplot(data=Houses[train.Id,], aes(x=log(Houses$SalePrice[train.Id]))) + geom_histogram(col="red", fill="green")
```
Some Feature Engineering
```{r}
Houses$TotalSF <- Houses$TotalBsmtSF + Houses$X2ndFlrSF + Houses$X1stFlrSF
Houses$Has2ndFlr <- factor(ifelse(Houses$X2ndFlrSF > 0, "yes", "no"), levels = c("yes","no"))

```


These predictors are really qualitative and not quantitative.
```{r}
Houses$MSSubClass <- factor(Houses$MSSubClass)
Houses$MoSold <- factor(Houses$MoSold)

Houses$KitchenAbvGr <- factor(Houses$KitchenAbvGr)


```
#### Porch Variables
Only Open Porch has a correlation that is not really low, so it makes sense to combine these variables then create a factor variable indicating the presense of each type of porch, other than the 3 seasons one as it is most likely insignificant, then also turn Total Porch Area using the cut function.
```{r}
Houses$TotalPorchArea <- Houses$X3SsnPorch + Houses$OpenPorchSF + Houses$ScreenPorch + Houses$EnclosedPorch
Houses$HasOpenPorch <- factor(ifelse(Houses$OpenPorchSF > 0, "yes", "no"), levels = c("yes","no"))
Houses$HasEnclosedPorch <- factor(ifelse(Houses$EnclosedPorch > 0, "yes", "no"), levels = c("yes","no"))
Houses$HasScreenPorch <- factor(ifelse(Houses$ScreenPorch > 0, "yes", "no"), levels = c("yes","no"))

```
```{r}
summary(aov(log(SalePrice)~HasEnclosedPorch+HasOpenPorch+HasScreenPorch+TotalPorchArea*OpenPorchSF, data = Houses[train.Id,]))
```

```{r}
continuous.var.names <- c("YrSold","MiscVal","PoolArea","ScreenPorch","X3SsnPorch","EnclosedPorch", "OpenPorchSF","WoodDeckSF","GarageArea","GarageYrBlt", "GrLivArea","X2ndFlrSF","X1stFlrSF","TotalBsmtSF", "BsmtUnfSF", "BsmtFinSF1","MasVnrArea","YearRemodAdd","YearBuilt","LotArea","LotFrontage","TotalSF","TotalPorchArea")

for(i in 1:length(continuous.var.names)){
  print(continuous.var.names[i])
  print(x=cor(log(Houses$SalePrice[train.Id]), y=as.numeric(Houses[train.Id, continuous.var.names[i]])))
   }
```
```{r}
`%notin%` <- Negate(`%in%`)
categorical.var.names <- colnames(Houses)[colnames(Houses) %notin% continuous.var.names]
for(i in 1:length(categorical.var.names)){
  print(categorical.var.names[i])
  print(summary(aov(log(Houses$SalePrice[train.Id])~Houses[train.Id,categorical.var.names[i]]))) }
```
The following variables either have a very low Pearsons correlation coefficient if continuous, or fail to reject the null hypothesis of no correlation in a one way ANOVA test, if categorical. So we drop them.

```{r}
Houses$YrSold <- NULL
Houses$MiscVal <- NULL
Houses$MoSold <- NULL
Houses$LandSlope <- NULL

continuous.var.names <- continuous.var.names[continuous.var.names %notin% c("YrSold","MiscVal")]
categorical.var.names <- categorical.var.names[categorical.var.names %notin% c("MoSold","LandSlope")]
```
These variables seem better off by turning them into factors as well.
```{r}
Houses$OverallCond <- factor(Houses$OverallCond)

continuous.var.names <- continuous.var.names[continuous.var.names != "OverallCond"]
categorical.var.names <- c(categorical.var.names, "YearBuilt", "YearRemodAdd", "GarageYrBlt")
```
####Overall Cond
becomes significant after transforming to factor
```{r}
summary(aov(log(SalePrice)~OverallCond, data = Houses[train.Id,]))
```




####PoolArea
PoolArea has a very low correlation with log(SalePrice), but houses with a pool do seem to have a higher price, so a dummy will be created to determine if a house has a pool or not. However a one way anova table shows a small F value and large enough p value that suggests that this variable is not significant, so it is also dropped
```{r}
Houses$HasPool <- factor(ifelse(Houses$PoolArea > 0, "yes", "no"), levels = c("no","yes"))
Houses$PoolArea <- NULL
continuous.var.names <- continuous.var.names[continuous.var.names != "PoolArea"]
summary(aov(log(SalePrice)~HasPool, data = Houses[train.Id,]))

```
```{r}
Houses$HasPool <- NULL
```





## Building the model
First transform the continuous predictors
```{r}
Houses[,continuous.var.names] <- Houses[,continuous.var.names]+1

for(i in 1:length(continuous.var.names)){
  bxcx <- BoxCoxTrans(Houses[,continuous.var.names[i]])
  print(continuous.var.names[i])
  print(bxcx$lambda)}
```
Next build an appropriate matrix, first without using the transformation
```{r}
options(na.action = "na.pass")
var.mat <- model.matrix(log(SalePrice) ~  ., data = Houses)[,-1]
test.Id <- 1461:(nrow(test)+1460)

```

####Lasso
```{r}

lassoCV <- cv.glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), alpha = 1)
best.lambda <- lassoCV$lambda.min
fit.lasso <- glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), lambda = best.lambda, alpha = 1)
pred.lasso <- predict(fit.lasso, newx =  var.mat[-train.Id,], s=best.lambda)
TruePred <- exp(pred.lasso)
testpred <- data.frame(id = test.Id, SalePrice = TruePred )
colnames(testpred) <- c("id","SalePrice")
write.csv(testpred, file = "lassoT.csv", row.names = F)
print(best.lambda)
```


#### Ridge
```{r}
RidgeCV <- cv.glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), alpha = 0)
best.lambda <- RidgeCV$lambda.min
print(best.lambda)
fit.ridge <- glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), lambda = best.lambda, alpha = 0)
pred.ridge <- predict(fit.ridge, newx =  var.mat[-train.Id,], s=best.lambda)

TruePred <- exp(pred.ridge)
testpred <- data.frame(id = test.Id, SalePrice = TruePred )
colnames(testpred) <- c("id","SalePrice")
write.csv(testpred, file = "ridgeT.csv", row.names = F)
```
#### Best Other Elastic Net fit
```{r}
ElasticNetCV <- cva.glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), alpha = seq(.05,.95,.05)^3)
```
```{r}
bestlambd.vals <- c(0.05687804,0.05687804,0.05648373,0.05504825,0.05405574,0.0453852,0.03442563,0.02777884, 0.02141217,0.0206348,0.0186737,0.0143835,0.01131301,0.009057832,0.007364353,0.00606804,0.005058964,0.004261778, 0.003623661)
CVMs <- c(0.01436809,0.01433757,0.01426546,0.01415671,0.01401907,0.01389331,0.01381043,0.01375485,0.01371582, 0.01369296,0.01366774,0.01365200,0.01365405,0.01364978,0.01364714,0.01364892,0.01365276,0.01365330,0.01365587) 
which.min(CVMs)
min(bestlambd.vals)
```
so $\alpha = 0.421875$ and $\lambda = 0.003623661$
```{r}
fit.ElasticNet <- glmnet(var.mat[train.Id,], log(Houses$SalePrice[train.Id]), lambda = 0.003623661, alpha = 0.421875)
pred.ElasticNet <- predict(fit.ElasticNet, newx =  var.mat[-train.Id,], s=0.003623661)

TruePred <- exp(pred.ElasticNet)
testpred <- data.frame(id = test.Id, SalePrice = TruePred )
colnames(testpred) <- c("id","SalePrice")
write.csv(testpred, file = "EN.csv", row.names = F)
```
I have found that the transformations did not improve the result, so they are not included.
